{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5.2 -- Scraping IMBD\n",
    "\n",
    "Our goal is to scrap [IMDB](imdb.com) user reviews for *Borat Subsequent Moviefilm*.  Unfortunately, the page for user reviews only shows a limited number of reviews and you can't access additional pages through a link.  `selenium` to the rescue! In this lab, we will combine our two approaches to web scraping by\n",
    "\n",
    "1. Using `selenium` to load the page and click the *Load More* until we have all the reviews.\n",
    "2. Creating a `BeautifulSoup` instance for the complete page and parsing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 -- Load the reviews.\n",
    "\n",
    "Explore IMBD to find the web link for the user reviews for *Borat Subsequent Moviefilm* and load this page in Python with `selenium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/dm6258xw/Downloads/chromedriver\n"
     ]
    }
   ],
   "source": [
    "#Easy way to copy and paste the filepath\n",
    "import easygui\n",
    "file = easygui.fileopenbox()\n",
    "print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome(executable_path=file)\n",
    "driver.get('https://www.imdb.com/title/tt13143964/reviews?ref_=tt_ov_rt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 -- Figure out how to click the *Load More* button.\n",
    "\n",
    "To load all of the user reviews, we need to click the *Load More* button multiple times.  First, find the corresponding WebElement and verify that clicking this button loads another page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "driver.find_element_by_class_name('ipl-load-more__button').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 -- Click *Load More* until you have all the results.\n",
    "\n",
    "Now you need to write code that will keep clicking the *Load More* button when you find it.  **Hint:** We can think of this as an example of an *unfold* process, meaning you should use a `while` loop combined with a [try-and-except statement](https://pythonbasics.org/try-except/) to keep trying to click the button.  To make sure you don't get an infinite loop, use a variable to identify and hold the stopping condition/state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The loop is now complete\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "stopping_condition = 0\n",
    "\n",
    "while(stopping_condition != 1):\n",
    "    try:\n",
    "        driver.find_element_by_class_name('ipl-load-more__button').click()\n",
    "        sleep(1) #To allow the next button to appear\n",
    "    except:\n",
    "        stopping_condition = 1\n",
    "        print('The loop is now complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 -- Load the results in a `BeautifulSoup` object.\n",
    "\n",
    "Since `bs4` has better tools for parsing html, we will now switch to using this module to parse the results.  Recall that you can access the content of the current content from the `selenium` driver using `driver.page_source`.  You can use this attribute to make a `soup` object for the page using \n",
    "\n",
    "> soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from composablesoup import get_text\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 -- Extract the information\n",
    "\n",
    "Now extract the following data to a csv file.\n",
    "\n",
    "1. Title\n",
    "2. Score\n",
    "3. User\n",
    "4. Date\n",
    "5. Text (replace commas with semi-colons!)\n",
    "6. Two columns for X and Y, where `\"X out of Y found this helpful\"`\n",
    "7. Permanent link the the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[' Borat Make a Number 2',\n",
       " ' Laugh Out Loud Funny S#!^!',\n",
       " ' Excellent. And this is from a non Sasha Cohen Baron fan. REAL REVIEW.',\n",
       " ' Cohen is a genius',\n",
       " \" The 10's are 10's & The 1's are 10's!\"]"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "# Titles\n",
    "\n",
    "titles = soup.find_all('a', attrs={'class':'title'})\n",
    "title_messy = [get_text(x, strip=True) for x in titles]\n",
    "title_output = [x.replace(',',';') for x in [x.replace('\\n', '') for x in title_messy]]\n",
    "\n",
    "title_output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['10', '10', '10', '10', '10']"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "# Scores\n",
    "score_re = re.compile('^\\d\\d?$')\n",
    "scores = soup.find_all('span')\n",
    "score_text_all = [get_text(x, strip=True) for x in scores]\n",
    "score_output = list(filter(score_re.match, score_text_all))\n",
    "\n",
    "score_output[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['MissCzarChasm',\n",
       " 'YourSonsDad',\n",
       " 'lvanka',\n",
       " 'WindsOfWintergreen',\n",
       " 'AnaAnaBanana']"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "# Users\n",
    "users = soup.find_all('span', attrs={'class':'display-name-link'})\n",
    "user_output = [get_text(x, strip=True) for x in users]\n",
    "\n",
    "user_output[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date\n",
    "dates = soup.find_all('span', attrs={'class':'review-date'})\n",
    "date_output = [get_text(x, strip=True) for x in dates]\n",
    "\n",
    "#Text\n",
    "texts = soup.find_all('div', attrs={'class':'text show-more__control'})\n",
    "text = [get_text(x, strip=True) for x in texts]\n",
    "text_output = [x.replace('\\n', ' ') for x in [x.replace(',', ';') for x in text]]\n",
    "\n",
    "#X out of Y\n",
    "found_helpful = soup.find_all('div', attrs={'class':'actions text-muted'})\n",
    "found_helpful_text = [get_text(x, strip=True) for x in found_helpful]\n",
    "found_helpful_output_combined = [re.findall('[0-9]+', x) for x in found_helpful_text]\n",
    "\n",
    "found_helpful_output = [', '.join(x) for x in found_helpful_output_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.imdb.com/review/rw6217081/?ref_=tt_urv',\n",
       " 'https://www.imdb.com/review/rw6217081/?ref_=tt_urv',\n",
       " 'https://www.imdb.com/review/rw6213611/?ref_=tt_urv',\n",
       " 'https://www.imdb.com/review/rw6213611/?ref_=tt_urv',\n",
       " 'https://www.imdb.com/review/rw6219436/?ref_=tt_urv']"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "#Permalink\n",
    "href = re.compile('/review/.*')\n",
    "all_href = [a['href'] for a in soup.select('a[href]')]\n",
    "permalink = ['https://www.imdb.com' + x for x in all_href if href.match(x)]\n",
    "\n",
    "permalink[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [list(a) for a in zip(title_output, score_output, user_output, date_output, text_output,found_helpful_output,permalink)]\n",
    "\n",
    "comma_join = [','.join(x) for x in results_list]\n",
    "\n",
    "line_join = '\\n'.join(comma_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lab5_2.csv', 'w') as outfile:\n",
    "    outfile.write(line_join)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}